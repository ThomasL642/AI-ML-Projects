{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training a Lunar Lander to Land with a DQN",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "lQaSnC2tmiA1",
        "tGz9jZYjIx1K",
        "23Nt2maILBOS",
        "vqcLtW1ILthR",
        "Ah4GgWtvTdwX",
        "nSuPOOFTVjC2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasL642/AI-ML-Projects/blob/main/Training_a_Lunar_Lander_to_Land_with_a_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQaSnC2tmiA1"
      },
      "source": [
        "#**Importing Libaries and Display Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGz9jZYjIx1K"
      },
      "source": [
        "###**Importing Packages for Video Display**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23Nt2maILBOS"
      },
      "source": [
        "###**Install Packages for Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[Box2d] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqcLtW1ILthR"
      },
      "source": [
        "###**Installing Packages for display and Packages for the Model and Enviroment**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #This will Set the minimal amount of logger message to 40 so we only see error messages\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import Box2D\n",
        "import random\n",
        "from keras.layers import Activation, Dense\n",
        "from keras import Sequential\n",
        "from collections import deque\n",
        "from keras.optimizers import Adam\n",
        "from keras.activations import relu, linear"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah4GgWtvTdwX"
      },
      "source": [
        "###**Defining the Display**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900)) #Creating a screen that is 1400p long and 900p tall\n",
        "display.start() #Starting the display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSuPOOFTVjC2"
      },
      "source": [
        "###**Display**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D"
      },
      "source": [
        "#This cell is not my code it's just the loop to display the video for the open AI gym enviroments\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4KIYnT5k31i"
      },
      "source": [
        "#**Deep Q Learning time!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WVlAe8I1HN3"
      },
      "source": [
        "This cell we are **calling the enviroment** with our video wrapper so we can see the environment ;and also, so we can use the environment to train our agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wmIBczjmegh"
      },
      "source": [
        "env = wrap_env(gym.make(\"LunarLander-v2\"))\n",
        "env.seed(111)\n",
        "np.random.seed(111) #making the results set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuImhsZY0c3c"
      },
      "source": [
        "###**Define the Agent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otwPbbZ03kaX"
      },
      "source": [
        "This cell we're defining the agent class. I'll explain each part in depth with comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafKvehLnBFm"
      },
      "source": [
        "class DQN:\n",
        "\n",
        "\tdef __init__(self, action_space, state_space): #__init__ is the constructor every time DQN is called it will be called\n",
        "   \n",
        "\t\tself.action_space = action_space #the action space is the space where all valid actions take place\n",
        "\t\tself.state_space = state_space #the state space is all possible states the agent can be in\n",
        "\t\tself.epsilon = 1.1 #the epsilon is a value that changes how much exploring the agent will do. The higher the value, the more agent explores the environment.\n",
        "\t\tself.gamma = 0.99 #the gamma is a value from 0 to 1 that places higher value on long term rewards. The higher the value, the more it value long term rewards.\n",
        "\t\tself.batch_size = 50 #the batch size is how many samples the model must run through to update its parameters.\n",
        "\t\tself.epsilon_min = 0.02 #this value is the lowest the epsilon can go\n",
        "\t\tself.lr = 0.001 #this value is how fast the model tries to find the answer - so the higher the faster it tries to find the answer. This can lead to converging on false solutions.\n",
        "\t\tself.epsilon_decay = 0.995 #this value is how fast the epsilon decays \n",
        "\t\tself.memory = deque(maxlen = 1000000) #the memory is exactly what it sounds like, it is basically a log for state, action taken, reward given becuase of action, and the next state\n",
        "\t\tself.model = self.DQN_model() #this is defining the model as the build_DQN function\n",
        "  \n",
        "\tdef DQN_model(self): #making the model\n",
        "   \n",
        "\t\tmodel = Sequential() #Sequantial models that stack layers as they are ordered\n",
        "\t\tmodel.add(Dense(150, input_dim = self.state_space, activation=relu)) #Dense layer 1: inputting the state_space, relu makes negatives equal 0, 150 neurons\n",
        "\t\tmodel.add(Dense(120, activation=relu)) #Dense layer 2: relu again, 120 neurons\n",
        "\t\tmodel.add(Dense(self.action_space, activation=linear)) #self.action_space = amount of neurons, linear function \n",
        "\t\tmodel.compile(loss=\"mse\", optimizer=Adam(lr=self.lr)) #compile the model, mean squared error as loss which measures the mean of the squared error; hence, the name\n",
        "\t\treturn model\n",
        "\n",
        "\tdef new_memory(self, state, action, reward, next_state, done):\n",
        "\t\tself.memory.append((state, action, reward, next_state, done))\n",
        "    #here we're defining the new_memory function which adds new the states, actions taken, reward given, the next_states and if the environment is done\n",
        "\n",
        "\tdef predict(self, state): #we're defining the predict function now which will be what predicts what state to be in\n",
        "        #if a random array is less or equal to epsilon, return an element from action_space\n",
        "\t\tif np.random.rand() <= self.epsilon:\n",
        "\t\t\treturn random.randrange(self.action_space)\n",
        "\t\tpredict_values = self.model.predict(state) #action values = what the model predicts they are\n",
        "\t\treturn np.argmax(predict_values[0]) #we're returning the highest values of the first action array, the model predicts.\n",
        "\n",
        "\tdef replay(self): #we're defining the replay memory function\n",
        "    \n",
        "\t\tif len(self.memory) < self.batch_size: #if amount of memory tuples is less than batch size we'll have duplicates\n",
        "                                              #but if the amount of memory tuples is more than the batch size we'll have no duplicates\n",
        "\t\t\treturn\n",
        "\n",
        "\t\tsample = random.sample(self.memory, self.batch_size) #sample is a random point in the memory the amount of batches they are \n",
        "\t\t\n",
        "\t\tstates = np.array([i[0] for i in sample]) #for the states array we are taking the amount of batch size random samples then iterating though that array\n",
        "\t\tactions = np.array([i[1] for i in sample]) #for the actions array we are taking the amount of batch size random samples then iterating though that array\n",
        "\t\trewards = np.array([i[2] for i in sample]) #for the rewards array we are taking the amount of batch size random samples then iterating though that array\n",
        "\t\tnext_states = np.array([i[3] for i in sample]) #for the next_states array we are taking the amount of batch size random samples then iterating though that array\n",
        "\t\tdones = np.array([i[4] for i in sample]) #for the dones array we are taking the amount of batch size random samples then iterating though that array\n",
        "\n",
        "\t\tstates = np.squeeze(states)#converting the 3D state array but a 2D array\n",
        "\t\tnext_states = np.squeeze(next_states)#converting the 3D next_state array to a 2D array\n",
        "\t\tQtargets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones) # Qtragets = reward of action in that state + discounted max q value in all possible actions for thatr state\n",
        "\t\tQtarget = self.model.predict_on_batch(states) #Qtraget = model trying to predict the states\n",
        "\t\tbatch_size_array = np.array([i for i in range(self.batch_size)]) #make a array from 0 batch_size -1 So if batch size 5 array will be  [0, 1, 2, 3, 4]\n",
        "\t\tQtarget[[batch_size_array], [actions]] = Qtargets #finding the batch_size_array and actions array in the Qtraget array \n",
        "\n",
        "\t\tself.model.fit(states, Qtarget, epochs=1, verbose=0) #training the model on the states, Qtarget, for for epochs cause each episode is on epoch and verbose is just the progress bar setting\n",
        "\t\tif self.epsilon > self.epsilon_min: #if epsilon is greater than the min we decrease the epsilon by time it by epsilon decay\n",
        "\t\t\tself.epsilon *= self.epsilon_decay\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0zq5sVt_zEM"
      },
      "source": [
        "To connect all the dots in this cell we created an DQN agent\n",
        "\n",
        "- That has certain values for learning like epsilon, gamma, espilon decay, learning rate \n",
        "- Created the Neural Network\n",
        "- Has a empty memory of state, reward, action, next action and if done, can add to that memory\n",
        "- Can predict the state to be in\n",
        "- train on its rewards of its prediction + discount q value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhf73-du0VSM"
      },
      "source": [
        "###**Defining Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxPoM4Jt79o9"
      },
      "source": [
        "def train_dqn(episode):#def the train function\n",
        "\n",
        "\tloss = [] #making loss a list\n",
        "\n",
        "\tagent = DQN(env.action_space.n, env.observation_space.shape[0]) #calling agent in action_space and observable space as a integer\n",
        "\tfor current_episode in range(episode): #making the training loop, for each episode run this loop\n",
        "\t\tstate = env.reset() #reset the environment + state = environment\n",
        "\t\tstate = np.reshape(state, (1, 8)) #reshaping the environment/state to not take hours to train\n",
        "\t\tscore = 0 #defining the score and setting it to 0\n",
        "\t\tmax_actions = 3000 #setting the maxing amount of actions\n",
        "\t\tfor i in range(max_actions): #action loop\n",
        "\t\t\taction = agent.predict(state) #action is the predict function defined early\n",
        "\t\t\tenv.render() #render the enviroment\n",
        "\t\t\tnext_state, reward, done, ii = env.step(action) #next_state, the reward, and if it's done are what make the enviroment step which is bassically actions can take place in the enivroment\n",
        "\t\t\tscore += reward #the score is equal to the score + reward\n",
        "\t\t\tnext_state = np.reshape(next_state, (1, 8)) #reshaping the next state to be smaller\n",
        "\t\t\tagent.new_memory(state, action, reward, next_state, done) #runnning the new_memory function defined earlier to append the state, action, reward, next_state and done\n",
        "\t\t\tstate = next_state #making state equal the next_state \n",
        "\t\t\tagent.replay() #running the replay function defined earlier\n",
        "            \n",
        "\t\t\tif done: #if the environment is done print f string and break from loop\n",
        "\t\t\t\tprint(f\"Episode {current_episode}/{episode}, Score: {score}\")\n",
        "\t\t\t\tbreak\n",
        "\t\tloss.append(score) #add score to loss list\n",
        "   \n",
        "\t\tsolved = np.mean(loss[-100:]) #solved equals mean of last 100 episodes\n",
        "\t\tif solved > 210: #if the average of 100 episodes is greater than 210, print f string and break\n",
        "\t\t\tprint(f\"We have Landed on Episode #{current_episode}\")\n",
        "\t\t\tbreak\n",
        "\t\tif current_episode > 99:\n",
        "\t\t\tprint(f\"Average Loss Over the Past 100 Episode: {solved}\") #print f string if episode is greater than 99\n",
        "\t\telse:\n",
        "\t\t\tprint(f\"Average Loss Over the Past {current_episode} Episode: {solved}\")#print this f string otherwise\n",
        "\treturn loss #sending loss back to caller"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LghRDDsEDK6"
      },
      "source": [
        "To connect the dots we are creating loss, calling agent, then for an episode:\n",
        "- reset environment\n",
        "- resetting the score\n",
        "- reshaping the environment\n",
        "- setting max actions\n",
        "\n",
        "Then for each action we are:\n",
        "- predicting the state\n",
        "- render the environment\n",
        "- doing that action\n",
        "- adding state, reward, action, next_state and if done to memory\n",
        "- training the agent\n",
        "\n",
        "Then breaking the loop - if done then:\n",
        "- append score\n",
        "- print information\n",
        "- breaking loop if all episodes are done or if solved\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq6colFr0Beu"
      },
      "source": [
        "###**Training Time!**\n",
        "\n",
        "Here we are printing infomation, setting amount of episodes to train for, training, plotting infomation and showing the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6kf9K_PnWkK"
      },
      "source": [
        "print(env.observation_space) #print observation space\n",
        "print(env.action_space) #print action space\n",
        "episodes = 1000 #train for this many episodes\n",
        "loss = train_dqn(episodes) #loss = train function\n",
        "plt.plot([i+1 for i in range(0, len(loss), 2)], loss[::2]) #plotting loss over time\n",
        "plt.show() #show graph\n",
        "show_video() #show the video"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}