{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Celeb_A GAN ",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16GIYw9gFZe_zjOP0dHq7pgU4Xps7o1Dx",
      "authorship_tag": "ABX9TyPZAdSchRylGBX0EtkLiotW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasL642/Thomas/blob/main/Celeb_A_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byqr15FsQFBh"
      },
      "source": [
        "import os\r\n",
        "import keras\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "from tqdm import tqdm\r\n",
        "import tensorflow as tf\r\n",
        "import glob\r\n",
        "\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.layers import Input\r\n",
        "from keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Dropout, Reshape, Flatten, Cropping2D, BatchNormalization\r\n",
        "from keras.layers.advanced_activations import LeakyReLU\r\n",
        "from keras import initializers\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.layers.convolutional import Conv2DTranspose, Conv2D\r\n",
        "\r\n",
        "np.random.seed(10)\r\n",
        "random_dim = 100\r\n",
        "DATADIR = \"/content/drive/MyDrive/Colab Notebooks/DATA/Celeb data/img_align_celeba\"\r\n",
        "IMG_SIZE = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnPNM4rjo-MC"
      },
      "source": [
        "#training_data = []\r\n",
        "\r\n",
        "#images = glob.glob('/content/drive/MyDrive/Colab Notebooks/DATA/Celeb data/img_align_celeba/*.jpg')\r\n",
        "#def create_training_data():\r\n",
        "#  for img in tqdm(os.listdir(DATADIR)):\r\n",
        "#    image = cv2.imread(img)\r\n",
        "#    img_array = cv2.imread(os.path.join(DATADIR,img))\r\n",
        "#    new_array = cv2.resize(img_array,(IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\r\n",
        "#    training_data.append(new_array)\r\n",
        "#    plt.imshow(training_data[0])\r\n",
        "#    plt.show\r\n",
        "\r\n",
        "#create_training_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wusx800P-WFM"
      },
      "source": [
        "x_train = np.load('/content/drive/MyDrive/Colab Notebooks/DATA/CelebANumpy.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccA-erj6WCt1"
      },
      "source": [
        "#print(len(training_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YJwyJ6LqILK"
      },
      "source": [
        "def load_data():\r\n",
        "  x_train = x_train\r\n",
        "  x_train = (x_train.astype(np.float32) - 127.5)/127.5\r\n",
        "  return x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKPTkOIIqZv5"
      },
      "source": [
        "def get_optimizer():\r\n",
        "  return Adam(lr=0.00001, beta_1=0.9)\r\n",
        "\r\n",
        "def get_generator(optimizer):\r\n",
        "  alpha = 0.2\r\n",
        "\r\n",
        "  generator = Sequential()\r\n",
        "  generator.add(Dense(2*2*512, input_dim = random_dim,kernel_initializer=initializers.RandomNormal(stddev=0.02)))\r\n",
        "\r\n",
        "  generator.add(Reshape((-1, 2, 2, 512)))\r\n",
        "  generator.add(BatchNormalization)\r\n",
        "  generator.add(Maximum(alpha = 0.2))\r\n",
        "\r\n",
        "  generator.add(Conv2DTranspose(256, 5, 2, padding=\"VALID\"))\r\n",
        "  generator.add(BatchNormalization())\r\n",
        "  generator.add(Maximum(alpha = 0.2))\r\n",
        "  \r\n",
        "  generator.add(Conv2DTranspose(128, 5, 2, padding=\"SAME\"))\r\n",
        "  generator.add(BatchNormalization())\r\n",
        "  generator.add(Maximum(alpha = 0.2))\r\n",
        "\r\n",
        "  generator.add(Conv2DTranspose(out_channel_dim, 5, 2, padding=\"SAME\", activatins = \"tanh\"))\r\n",
        "  generator.compile(loss=\"BinaryCrossentropy\", optimizer=optimizer)\r\n",
        "\r\n",
        "  return generator\r\n",
        "\r\n",
        "def get_discriminator(optimizer):\r\n",
        "\r\n",
        "  alpha = 0.2\r\n",
        "  discriminator = Sequential()\r\n",
        "\r\n",
        "  discriminator.add(Conv2D(images, 64, 5, 2, \"SAME\"))\r\n",
        "  discriminator.add(Maximum(alpha = 0.2))\r\n",
        "\r\n",
        "  discriminator.add(Conv2D(128, 5, 2, \"SAME\"))\r\n",
        "  discriminator.add(BatchNormalization())\r\n",
        "  discriminator.add(Maximum(alpha = 0.2))\r\n",
        "\r\n",
        "  discriminator.add(Conv2D(lrelu2, 256, 5, 1, \"SAME\"))\r\n",
        "  discriminator.add(BatchNormalization())\r\n",
        "  discriminator.add(Maximum(alpha = 0.2)) \r\n",
        "\r\n",
        "  discriminator.add(Reshape((-1, 4*4*256)))\r\n",
        "\r\n",
        "  discriminator.add(Dense(1, activation = \"sigmoid\"))\r\n",
        "  discriminator.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\r\n",
        "\r\n",
        "  return discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiq235OGOzZp"
      },
      "source": [
        "def get_gan_network(discriminator, random_dim, generator, optimizer):\r\n",
        "  discriminator.trainable = False\r\n",
        "  gan_input = Input(shape=(random_dim,))\r\n",
        "  x = generator(gan_input)\r\n",
        "  gan_output = discriminator(x)\r\n",
        "  gan = Model(inputs=gan_input, outputs=gan_output)\r\n",
        "  gan.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\r\n",
        "  return gan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmAoz0dtO7lY"
      },
      "source": [
        "def plot_generated_images(epoch, generator, examples=100, dim=(10, 10), figsize=(10, 10)):\r\n",
        "  noise = np.random.normal(0, 1, size=[examples, random_dim])\r\n",
        "  generated_images = generator.predict(noise)\r\n",
        "  generated_images = generated_images.reshape(examples, , 28)\r\n",
        "\r\n",
        "  plt.figure(figsize=figsize)\r\n",
        "  for i in range(generated_images.shape[0]):\r\n",
        "    plt.subplot(dim[0], dim[1], i+1)\r\n",
        "    plt.imshow(generated_images[i], interpolation=\"nearest\")\r\n",
        "    plt.axis(\"off\")\r\n",
        "  plt.tight_layout()\r\n",
        "  plt.savefig(\"gan_generated_image_epoch_%d.png\" % epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjW_j1rj6QFU"
      },
      "source": [
        "def train(epochs=1, batch_size=50):\r\n",
        "  batch_count = x_train.shape[0] / batch_size\r\n",
        "  adam = get_optimizer()\r\n",
        "  generator = get_generator(adam)\r\n",
        "  discriminator = get_discriminator(adam)\r\n",
        "  gan = get_gan_network(discriminator, random_dim, generator, adam)\r\n",
        "  #random noise and images\r\n",
        "  for e in range(1, epochs+1):\r\n",
        "    print(\"-\"*15,\"Epoch %d\" % e, \"-\"*15)\r\n",
        "    for _ in tqdm(range(int(batch_count))):\r\n",
        "      noise = np.random.normal(0, 1, size=[batch_size, random_dim])\r\n",
        "      image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\r\n",
        "      \r\n",
        "      #generate fake images\r\n",
        "      generated_images = generator.predict(noise)\r\n",
        "      X = np.concatenate([image_batch, generated_images])\r\n",
        "\r\n",
        "      #labels for fake + real\r\n",
        "      y_dis = np.zeros(2*batch_size)\r\n",
        "      y_dis[:batch_size] = 0.9\r\n",
        "\r\n",
        "      discriminator.trainable = True \r\n",
        "      discriminator.train_on_batch(X, y_dis)\r\n",
        "\r\n",
        "      noise = np.random.normal(0, 1, size=[batch_size, random_dim])\r\n",
        "      y_gen = np.ones(batch_size)\r\n",
        "      discriminator.trainable = False\r\n",
        "      gan.train_on_batch(noise, y_gen)\r\n",
        "\r\n",
        "    if e == 1 or e % 20 == 0:\r\n",
        "      plot_generated_images(e, generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGVwMU7nHCqt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "0b487238-6d0b-4a41-8c31-18808b317168"
      },
      "source": [
        "train(600, 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ce62cfc5cc39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-92be004767f8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mbatch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0madam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gan_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f34ee994c9f0>\u001b[0m in \u001b[0;36mget_generator\u001b[0;34m(optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    182\u001b[0m       raise TypeError('The added layer must be '\n\u001b[1;32m    183\u001b[0m                       \u001b[0;34m'an instance of class Layer. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                       'Found: ' + str(layer))\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: <class 'tensorflow.python.keras.layers.normalization_v2.BatchNormalization'>"
          ]
        }
      ]
    }
  ]
}